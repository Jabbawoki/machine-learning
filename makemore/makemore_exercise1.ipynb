{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b03a099-bc30-4999-a884-30ef9bd818fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "# E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "# E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "# E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "# E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcee7a-20af-4a3f-af12-fd8b85720db0",
   "metadata": {},
   "source": [
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d38e4f-14cc-4908-8aa4-7600ca6a63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ae899356-6c7d-4325-860c-61efc6596589",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "905ba3e5-e92a-42f2-b536-34cc5ec509e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given tri-gram add another dimension\n",
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# Character number: a : 1\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "# Number character: 1 : a\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0165a474-0ac2-4d54-a477-8bf74117cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTING METHOD APPROACH TO TRIGRAM\n",
    "log_likehood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        # Index which character and add to distribution model\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "        \n",
    "# Ensure no zeros in generation, therefore no infinity unlikely\n",
    "# Model smoothing by adding 1 to all values.\n",
    "P = (N+1).float()\n",
    " # .sum(2) because we want the third level of the array to be normalized to get the probabilities\n",
    "P /= P.sum(2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b3ccb788-447a-4ab9-acb3-affdb15c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bi-gram model\n",
    "\n",
    "# [ \n",
    "#   a: [a => 0.4, b => 0.6] # We had a probability of 0.4 to get an \"a\" after an \"a\"\n",
    "#   b: [a => 0.3, b => 0.7]\n",
    "# ] \n",
    "\n",
    "# # Trigram model\n",
    "\n",
    "# [ \n",
    "#   a: [\n",
    "#     a: [a => 0.4, b => 0.6] # We had a probability of 0.4 to get an \"a\" after an \"a\" after an \"a\"\n",
    "#     b: [a => 0.3, b => 0.7]\n",
    "#   ]\n",
    "#   b: [\n",
    "#     a: [a => 0.8, b => 0.2]\n",
    "#   ]\n",
    "# ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "32244bc3-5ff9-4853-b1d6-ea1b12801e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check normalization was successfull\n",
    "# Sum of values in third level sums to 1 (normalized)\n",
    "P[1, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "77ab8341-0fe7-4462-a2f4-512c0943ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdunide.\n",
      "abilyasid.\n",
      "aburelay.\n",
      "abelin.\n",
      "ab.\n",
      "abdi.\n",
      "abritoper.\n",
      "abrayel.\n",
      "abetannaaryanileniassibiainewin.\n",
      "abressiyanayla.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Sample for probabilities\n",
    "for i in range(10):\n",
    "  # Change depending on what we want\n",
    "  out = ['a', 'b']\n",
    "  while True:\n",
    "    p = P[stoi[out[-2]], stoi[out[-1]]] # Plug the last two chars into our probabilities table\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "aceb2490-cc3e-4b02-99ec-bb58792e9cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples 196113\n"
     ]
    }
   ],
   "source": [
    "# Neural network time\n",
    "# create data\n",
    "# input, target\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2)) # Need to keep two chars for input, so array of tuple\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(\"examples\", ys.shape[0])\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W =  torch.randn((27*2, 27), generator=g, requires_grad=True) \n",
    "# You just need 27x2, the first 27 will activate for the first input, the second 27 for the second one\n",
    "# Double the inputs, double the neurons, that makes sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "2d7a8b8e-ae5f-40e2-8443-bd639f5cae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.283792018890381\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Sampling size and gradient descent.softmax classifier to find loss function\n",
    "for k in range(1000):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # does ont hot on both inputs, so get two times 27 tensors, does neuron\n",
    "    # .view() is used to change the format of tensors. xenc is currently [examplesCount, 2, 27]\n",
    "    # and we want [examplesCount, 54] (to match the shape of W [54, 27])\n",
    "    logits = xenc.view(-1, 27*2) @ W # -1 means we let torch define how much we'll get in the first dimension to stay compatible, in our case it should not change\n",
    "    \n",
    "    # SOFTMAX CLASSIFIER\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() # ys.shape[0] is the number of examples (allows us to avoid using a count variable)\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -3 * W.grad\n",
    "    \n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec6d4f-c827-4d8a-b75b-d6f2daa5a59d",
   "metadata": {},
   "source": [
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "543cfd7d-d657-4fd7-bce6-56bbd00f94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(words)\n",
    "c = len(words)\n",
    "\n",
    "trainSet = words[:math.floor(c * 0.8)]\n",
    "devSet = words[math.floor(c * 0.8): math.floor(c * 0.9)]\n",
    "testSet = words[math.floor(c * 0.9):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5d5e25ed-3374-4a28-a3d5-6e6f1f5c7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples 156870\n",
      "2.2842559814453125\n"
     ]
    }
   ],
   "source": [
    "# TRAIN SET\n",
    "\n",
    "xs, ys = [], []\n",
    "lossi = []\n",
    "for w in trainSet:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2)) # Need to keep two chars for input, so array of tuple\n",
    "        ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(\"examples\", ys.shape[0])\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W =  torch.randn((27*2, 27), generator=g, requires_grad=True) \n",
    "\n",
    "# Sampling size and gradient descent.softmax classifier to find loss function\n",
    "for k in range(1000):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # does ont hot on both inputs, so get two times 27 tensors, does neuron\n",
    "    # .view() is used to change the format of tensors. xenc is currently [examplesCount, 2, 27]\n",
    "    # and we want [examplesCount, 54] (to match the shape of W [54, 27])\n",
    "    logits = xenc.view(-1, 27*2) @ W # -1 means we let torch define how much we'll get in the first dimension to stay compatible, in our case it should not change\n",
    "    \n",
    "    # SOFTMAX CLASSIFIER\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() # ys.shape[0] is the number of examples (allows us to avoid using a count variable)\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -3 * W.grad\n",
    "    \n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ac843a41-f680-426c-bc9f-90402de16cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2846)\n",
      "tensor(2.2820)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the loss for the testSet && devSet ? \n",
    "# TEST SET\n",
    "# No training is done here, just comparing loss function against training set\n",
    "lossi = []\n",
    "for w in testSet:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "      \n",
    "    xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    lossi.append(-probs[0, ix3].log())\n",
    "print(torch.tensor(lossi).mean()) \n",
    "\n",
    "# DEV SET\n",
    "lossi = []\n",
    "for w in devSet:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "      \n",
    "    xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    lossi.append(-probs[0, ix3].log())\n",
    "print(torch.tensor(lossi).mean())\n",
    "\n",
    "# The losses are very similar, which means that the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17d3cf-e045-45c6-aeb7-ce95f55fd405",
   "metadata": {},
   "source": [
    "# E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "508d5159-a6c3-4a6b-a627-c1606ecfff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothness: 1.0 => loss train set: 2.965202569961548\n",
      "Smoothness: 1.0 => loss dev set: 2.570209264755249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Smoothing\n",
    "smoothness = [0, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "for i, smoothness in enumerate(smoothness):\n",
    "    W = torch.randn((27*2, 27), generator=g, requires_grad=True) \n",
    "    for k in range(100):\n",
    "        xenc = F.one_hot(xs, num_classes=27).float()\n",
    "        logits = xenc.view(-1, 27*2) @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        # standard negative log-likelihood loss\n",
    "        # By squaring W, the penalty grows faster as weights increase in magnitude.\n",
    "        # This discourages the optimizer from increasing weights excessively, \n",
    "        # acting as a form of L2 regularization to promote smoother, more generalizable models.\n",
    "        loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + smoothness*(W**2).mean()\n",
    "        \n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        \n",
    "        W.data += -3 * W.grad\n",
    "\n",
    "print(f\"Smoothness: {smoothness} => loss train set: {loss.item()}\")\n",
    "    \n",
    "lossi = []\n",
    "for w in devSet:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "      \n",
    "    xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    lossi.append(-probs[0, ix3].log())\n",
    "print(f\"Smoothness: {smoothness} => loss dev set: {torch.tensor(lossi).mean()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63716b9f-7287-4e85-a8d1-eb00dbb224d7",
   "metadata": {},
   "source": [
    "# E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e9b00cc4-b73e-4312-bdc0-98779f5c67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure how this works\n",
    "xenc_pre = torch.zeros(*xs.shape, 27)\n",
    "xenc_pre[torch.arange(xs.shape[0]), 0, xs[:,0]] = 1\n",
    "xenc_pre[torch.arange(xs.shape[0]), 1, xs[:,1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381ef01-88e8-4934-a4e4-731b531c7088",
   "metadata": {},
   "source": [
    "# E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "688843e4-a7cf-41fc-a958-d08446060278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2926, grad_fn=<AddBackward0>),\n",
       " tensor(2.2926, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc.view(-1, 27*2) @ W\n",
    "\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "loss, F.cross_entropy(logits, ys) + 0.01*(W**2).mean()\n",
    "\n",
    "# https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd66b5-ec21-4f6f-82e7-d20f83d1fcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca05d72-fb1a-41d3-a176-86c99ecf778d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
