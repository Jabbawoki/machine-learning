{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac2de3b-2837-4792-9ba1-1c50340dab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "# E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "# E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0793003-ae28-4bed-a2c4-6b7876af3cab",
   "metadata": {},
   "source": [
    "# E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f39c55-1f24-409b-a8a4-c12bb060f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77982975-4e17-4257-b613-26e7755268d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bcc9ec-4609-4936-8e7b-1bcdfebd45a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary of characters and integer mapping\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi)\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234828df-5967-450a-b088-10433114112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(stoi['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab81e1b-1922-4991-b2cd-7742f5e36c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Build data set\n",
    "\n",
    "block_size = 3 # context length\n",
    "def build_dataset(words):\n",
    "    X, Y = [], [] # input (context to predict next char), output (next char predicted)\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context) # [s, c, a] -> [m]\n",
    "            Y.append(ix)\n",
    "            # sliding window\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# training split, dev/validation split, test split\n",
    "# 80%, 10%, 10%\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28ae8dae-7f53-41c8-89b6-df524850a394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "dim = 10\n",
    "Xtr.shape, Ytr.shape\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, dim), generator=g) # 27 characters with 10 dimensional embedding\n",
    "W1 = torch.randn((dim*3, 200), generator=g) # 10 (embedding) * 3 (context) = 30, 200 neurons (up to us)\n",
    "b1 = torch.randn(200, generator=g) # randomly initialized\n",
    "W2 = torch.randn((200, 27), generator=g) # previous dimension of W1, number of char\n",
    "b2 = torch.randn(27, generator=g) # number of char\n",
    "parameters = [C, W1, b1, W2, b2] \n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac202f3a-d644-4adf-b629-63e149fa1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5089edc1-198e-413f-a08b-bd3149617c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step linearly to exponents of lr, 10^-3 and 1 (learning rate)\n",
    "lre = torch.linspace(-3, 0, 1000) # [-3.0000, -2.9960, -2.9920, ..., -0.0080, -0.0040, 0.0000]\n",
    "lrs = 10**lre # [0.0010, 0.0010, 0.0010, ..., 0.9978, 0.9982, 1.0000]\n",
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9298379-11b5-49b2-ad9d-fb627f3540ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;66;03m# update parameter value using gradient descent rule\u001b[39;00m\n\u001b[0;32m     22\u001b[0m stepi\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m---> 23\u001b[0m lossi\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mlog10()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# complete training loop\n",
    "for i in range(200000):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,)) # minibatch of 32 samples from Xtr\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # embedding lookup\n",
    "    h = torch.tanh(emb.view(-1, dim*3) @ W1 + b1) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # softmax and negative log likelihood\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # clear to prevent accumulation\n",
    "    loss.backward() # compute gradient of the loss with respect to all parameters\n",
    "\n",
    "    # update \n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters: \n",
    "        p.data += -lr * p.grad # update parameter value using gradient descent rule\n",
    "\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e169ea-487a-463c-be8f-61e83f7bfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to optimize: TODO\n",
    "# change number of neurons\n",
    "# dimensionality of embedding loo up table\n",
    "# number of characters feeding in context\n",
    "# how long running\n",
    "# learning rate\n",
    "# how it decays\n",
    "# batchsize\n",
    "\n",
    "# plt.plot(stepi, lossi)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24ff03-bd3f-4a89-8fe7-1ce7ec7401b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb = C[Xtr] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5d90e-38a2-40e0-a761-87abee784ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c79513-2545-4e45-b11c-2a688ab55784",
   "metadata": {},
   "source": [
    "# E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5184979-373e-4de3-95dc-37670dc0afb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53c8e5c-f224-4c48-9787-602021bd4c9b",
   "metadata": {},
   "source": [
    "# E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7ef49-7202-4a69-b8fc-e653a162f194",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
